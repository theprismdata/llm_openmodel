{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7765fea9-1119-4436-ac34-bff9ca01c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b7f964-44ce-482c-8892-0b3dc4c8ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397cf530-e51d-4664-a57f-12a3b8187bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_utilization():\n",
    "    if torch.cuda.is_available():\n",
    "        used_memory = torch.cuda.memory_allocated() / 1024 /1024\n",
    "        print(f\"GPU 메모리 사용량: {used_memory:.3f} GB\")\n",
    "    else:\n",
    "        print(\"GPU 사용 불가\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bbf3b4c-f2a3-416e-a06a-f31d39a3a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_id, peft=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id,cache_dir=\"./\" )\n",
    "    if peft is None:\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\",cache_dir=\"./\", device_map=device)\n",
    "    print_gpu_utilization()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea43bc-a1a1-4166-a7fb-d0a50f1190c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6ca24d-4f15-4b68-bca8-79e3b6675de5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 메모리 사용량: 0.000 GB\n"
     ]
    }
   ],
   "source": [
    "model_id = \"Qwen/Qwen2-0.5B-Instruct\" #GPU-1G\n",
    "#model_id = \"Qwen/Qwen2-1.5B-Instruct\" #GPU - 4G \n",
    "model, tokenizer = load_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c800c5-4401-4289-bbd5-6ec3bc16fed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "682f6c74-b12f-4242-82f3-956d457708c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"다음 문장에서 잘못된 문장을 수정해 줘\"\n",
    "'집행관리조직은 대체로 작은 규모이면서 최고정책결정자(총리, 대 통령 등)의 직접적 업무범위 내에 위치하거나 중앙정부의 중심(center of government)에 위치하고 있다. \n",
    "작은 규모의 조직은 상대적으로 신축적이 며 응집성이 강하지만 이와 함께 우수한 역량의 직원을 선발해야 하고, 관 주 등에서도 행정개혁의 일환이거나 이를 위한 pilot 프로그램으로 시도되었다.\n",
    "32 Beattie가 이끄는 노동당의 2004년 재선 이후 초선 정부 시 나타난 주요 부처의 실망스 러운 성과를 반성하고 보다 견고한 정책제안을 내각에 제시하기 위해 설치되었다 (Tiernan[2006]).\n",
    "33 2003년 Howard 수상에 의해 설치되었으며, 정부의 주요 정책들에 대한 집행 및 서비 스 전달을 촉진하는 역할과 범부처의 정책집행 진척과정을 보고하는 역할을 담당하였다 (Australian Department of Prime Minister and Cabinet[2013]).\n",
    "34 미국의 경우에도 [그림 3]가 같이 Obama 2기 정부에서 High Priority Performance Goals라는 집행관 리를 시도하였는데, 여기서는 개별적인 집행관리부서를 두지 않고, 기존의 OMB가 그 역할을 대신하고 있다.\n",
    "106 정책효과성 증대를 위한 집행과학에 관한 연구 리대상인 업무가 제한적⋅선택적이어야 한다는 제약이 있다.'\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a korean editor\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f7702-a619-4963-8860-289327b3301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7125bfe-5c11-4405-98ea-177b9458b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ec123-3d39-4f08-8d68-1e09fe78dfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
